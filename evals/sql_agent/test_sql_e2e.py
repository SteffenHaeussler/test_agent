"""SQL E2E tests using FastAPI endpoint."""

import time
from pathlib import Path

import pytest

from evals.llm_judge import JudgeCriteria, LLMJudge
from evals.utils import (
    get_model_info_for_test,
    load_yaml_fixtures,
    normalize_sql,
    save_test_report,
)
from src.agent.domain import events

current_path = Path(__file__).parent

# Load fixtures from YAML files using the utility function
fixtures = load_yaml_fixtures(current_path, "e2e")


class TestSQLEndToEnd:
    """SQL End-to-End evaluation tests using FastAPI endpoint."""

    def setup_method(self):
        """Initialize LLM Judge for evaluation."""
        self.judge = LLMJudge()

    def setup_class(self):
        """Setup report file."""
        self.results = []

    def teardown_class(self):
        """Save results to report file."""
        model_info = get_model_info_for_test("sql_e2e")
        save_test_report(self.results, "sql_e2e", model_info)

    def extract_final_response(self, session_id: str, test_notifications) -> str:
        """Extract the final response from collected notifications."""
        # Get all events sent to this session
        session_events = test_notifications.sent.get(session_id, [])

        # Find the final response event
        for event in reversed(session_events):  # Check from most recent
            if isinstance(event, events.Evaluation):
                summary = event.summary

                return summary.split("\n\nHere is the SQL query:\n\n")[-1]

        return ""

    @pytest.mark.parametrize(
        "fixture_name, fixture",
        [
            pytest.param(fixture_name, fixture, id=fixture_name)
            for fixture_name, fixture in fixtures.items()
        ],
    )
    def test_sql_e2e(self, fixture_name, fixture, test_client, test_notifications):
        """Run E2E test with optional LLM judge evaluation."""

        # Clear all previous notifications
        # Since sent is a defaultdict, we need to clear its contents properly
        for key in list(test_notifications.sent.keys()):
            del test_notifications.sent[key]

        question = fixture["question"]
        expected_sql = fixture["sql"]

        # Create session ID for this test
        session_id = f"test-sql-{fixture_name}"
        # headers = {"X-Session-ID": session_id}

        # Start timing
        start_time = time.time()

        # Make API request
        params = {"question": question, "q_id": fixture_name}
        headers = {"X-Session-ID": session_id}
        response = test_client.get("/query", params=params, headers=headers)

        # API should return processing status
        assert response.status_code == 200
        assert response.json()["status"] == "processing"

        # Wait for async processing to complete
        # You may need to adjust this based on typical processing time
        max_wait_time = 30  # Maximum wait time in seconds
        wait_interval = 1  # Check interval in seconds
        elapsed_time = 0

        while elapsed_time < max_wait_time:
            # Check if we have received a response event
            session_events = test_notifications.sent.get(session_id, [])
            has_response = any(
                isinstance(event, events.Response) or hasattr(event, "response")
                for event in session_events
            )

            if has_response:
                break

            time.sleep(wait_interval)
            elapsed_time += wait_interval

        # Extract SQL from SSE response
        actual_sql = self.extract_final_response(session_id, test_notifications)

        # Calculate execution time
        execution_time_ms = int((time.time() - start_time) * 1000)

        # Normalize SQL for comparison (basic normalization)

        # Use LLM Judge for evaluation
        criteria = JudgeCriteria(**fixture.get("judge_criteria", {}))
        judge_result = self.judge.evaluate(
            question=question,
            expected=normalize_sql(expected_sql),
            actual=normalize_sql(actual_sql) if actual_sql else "NO SQL GENERATED",
            criteria=criteria,
            test_type="sql_e2e",
        )

        # Add delay to avoid rate limiting
        time.sleep(1)

        # Record result
        result = {
            "test_name": fixture_name,
            "question": question,
            "expected": normalize_sql(expected_sql),
            "actual": normalize_sql(actual_sql) if actual_sql else "NO SQL GENERATED",
            "passed": judge_result.passed,
            "execution_time_ms": execution_time_ms,
            "overall_score": (
                judge_result.scores.accuracy
                + judge_result.scores.relevance
                + judge_result.scores.completeness
                + judge_result.scores.hallucination
            )
            / 4,
            "accuracy": judge_result.scores.accuracy,
            "relevance": judge_result.scores.relevance,
            "completeness": judge_result.scores.completeness,
            "hallucination": judge_result.scores.hallucination,
            "judge_assessment": judge_result.overall_assessment,
        }
        self.__class__.results.append(result)

        # Assert judge passed
        assert judge_result.passed, f"Judge failed: {judge_result.overall_assessment}"
