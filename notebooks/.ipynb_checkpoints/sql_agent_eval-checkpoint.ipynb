{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Agent E2E Evaluation Testing Notebook\n",
    "\n",
    "This notebook provides an interactive environment for testing and developing SQL agent evaluation scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup imports and environment\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Import required modules\n",
    "from src.agent import bootstrap\n",
    "from src.agent.adapters.adapter import RouterAdapter\n",
    "from src.agent.adapters.notifications import CollectingNotifications\n",
    "from src.agent.domain import commands, events\n",
    "from src.agent.service_layer import messagebus\n",
    "from evals.utils import load_database_schema, normalize_sql\n",
    "from evals.llm_judge import JudgeCriteria, LLMJudge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Database Schema\n",
    "\n",
    "The SQL agent needs the database schema to generate proper SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the schema\n",
    "schema_path = Path(\"../evals/sql_agent/schema/schema.json\")\n",
    "schema = load_database_schema(schema_path.parent, schema_path.name)\n",
    "\n",
    "# Display schema info\n",
    "print(f\"Database: {schema.database_name}\")\n",
    "print(f\"Tables: {len(schema.tables)}\")\n",
    "for table in schema.tables:\n",
    "    print(f\"\\n  Table: {table.name}\")\n",
    "    print(f\"  Columns: {', '.join([f'{col.name} ({col.data_type})' for col in table.columns[:5]])}{'...' if len(table.columns) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the SQL Agent System\n",
    "\n",
    "Set up the message bus with a collecting notifications handler to capture responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create notifications collector\n",
    "notifications = CollectingNotifications()\n",
    "\n",
    "# Bootstrap the message bus with router adapter\n",
    "adapter = RouterAdapter()\n",
    "bus = bootstrap.bootstrap(adapter=adapter, notifications=[notifications])\n",
    "\n",
    "print(\"SQL Agent system initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Functions to make testing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_notifications():\n",
    "    \"\"\"Clear all collected notifications.\"\"\"\n",
    "    for key in list(notifications.sent.keys()):\n",
    "        del notifications.sent[key]\n",
    "\n",
    "def extract_sql_response(session_id: str) -> str:\n",
    "    \"\"\"Extract the SQL query from the evaluation response.\"\"\"\n",
    "    session_events = notifications.sent.get(session_id, [])\n",
    "    \n",
    "    for event in reversed(session_events):\n",
    "        if isinstance(event, events.Evaluation):\n",
    "            summary = event.summary\n",
    "            # Extract SQL from the summary\n",
    "            if \"Here is the SQL query:\" in summary:\n",
    "                return summary.split(\"\\n\\nHere is the SQL query:\\n\\n\")[-1]\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def test_sql_question(question: str, expected_sql: str = None, session_id: str = None):\n",
    "    \"\"\"Test a SQL question and optionally evaluate against expected SQL.\"\"\"\n",
    "    if session_id is None:\n",
    "        session_id = f\"test-{int(time.time())}\"\n",
    "    \n",
    "    # Clear previous notifications\n",
    "    clear_notifications()\n",
    "    \n",
    "    # Create the question command\n",
    "    q_id = f\"notebook-{int(time.time())}\"\n",
    "    question_cmd = commands.Question(question=question, q_id=q_id)\n",
    "    \n",
    "    # Process through message bus\n",
    "    print(f\"Processing question: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    messagebus.handle(question_cmd, bus, session_id)\n",
    "    \n",
    "    # Wait for processing to complete\n",
    "    max_wait = 30\n",
    "    elapsed = 0\n",
    "    while elapsed < max_wait:\n",
    "        events_list = notifications.sent.get(session_id, [])\n",
    "        if any(isinstance(e, events.Evaluation) for e in events_list):\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "        elapsed += 0.5\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    # Extract SQL response\n",
    "    actual_sql = extract_sql_response(session_id)\n",
    "    \n",
    "    print(f\"\\nGenerated SQL:\\n{actual_sql}\")\n",
    "    print(f\"\\nExecution time: {execution_time:.2f}s\")\n",
    "    \n",
    "    # Print all events for debugging\n",
    "    print(f\"\\nEvents received: {[type(e).__name__ for e in notifications.sent.get(session_id, [])]}\")\n",
    "    \n",
    "    # If expected SQL provided, evaluate\n",
    "    if expected_sql:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"EVALUATION\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nExpected SQL:\\n{expected_sql}\")\n",
    "        \n",
    "        # Use LLM Judge\n",
    "        judge = LLMJudge()\n",
    "        criteria = JudgeCriteria()\n",
    "        \n",
    "        judge_result = judge.evaluate(\n",
    "            question=question,\n",
    "            expected=normalize_sql(expected_sql),\n",
    "            actual=normalize_sql(actual_sql) if actual_sql else \"NO SQL GENERATED\",\n",
    "            criteria=criteria,\n",
    "            test_type=\"sql_e2e_notebook\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nJudge Result: {'PASSED' if judge_result.passed else 'FAILED'}\")\n",
    "        print(f\"Scores:\")\n",
    "        print(f\"  - Accuracy: {judge_result.scores.accuracy}\")\n",
    "        print(f\"  - Relevance: {judge_result.scores.relevance}\")\n",
    "        print(f\"  - Completeness: {judge_result.scores.completeness}\")\n",
    "        print(f\"  - Hallucination: {judge_result.scores.hallucination}\")\n",
    "        print(f\"\\nAssessment: {judge_result.overall_assessment}\")\n",
    "        \n",
    "        return actual_sql, judge_result\n",
    "    \n",
    "    return actual_sql, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Basic SQL Questions\n",
    "\n",
    "Let's test some basic SQL generation scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Simple SELECT query\n",
    "sql, judge = test_sql_question(\n",
    "    question=\"Show me all customers from New York\",\n",
    "    expected_sql=\"SELECT * FROM customers WHERE city = 'New York'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: JOIN query\n",
    "sql, judge = test_sql_question(\n",
    "    question=\"Show me all orders with customer names\",\n",
    "    expected_sql=\"SELECT o.*, c.name FROM orders o JOIN customers c ON o.customer_id = c.id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Aggregation query\n",
    "sql, judge = test_sql_question(\n",
    "    question=\"What is the total revenue by product category?\",\n",
    "    expected_sql=\"SELECT category, SUM(revenue) as total_revenue FROM products GROUP BY category\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Test Scenarios\n",
    "\n",
    "Use this section to create and test your own SQL scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom test\n",
    "custom_question = \"Find the top 5 customers by total order value\"\n",
    "custom_expected_sql = \"\"\"\n",
    "SELECT c.id, c.name, SUM(o.total_amount) as total_order_value \n",
    "FROM customers c \n",
    "JOIN orders o ON c.id = o.customer_id \n",
    "GROUP BY c.id, c.name \n",
    "ORDER BY total_order_value DESC \n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "sql, judge = test_sql_question(\n",
    "    question=custom_question,\n",
    "    expected_sql=custom_expected_sql\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Testing\n",
    "\n",
    "Test multiple scenarios at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test scenarios\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"name\": \"simple_filter\",\n",
    "        \"question\": \"Show all active products\",\n",
    "        \"expected_sql\": \"SELECT * FROM products WHERE status = 'active'\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"date_filter\",\n",
    "        \"question\": \"Find orders from last month\",\n",
    "        \"expected_sql\": \"SELECT * FROM orders WHERE order_date >= DATE_SUB(CURRENT_DATE, INTERVAL 1 MONTH)\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"complex_join\",\n",
    "        \"question\": \"Show customer orders with product details\",\n",
    "        \"expected_sql\": \"\"\"\n",
    "            SELECT c.name as customer_name, o.order_date, p.name as product_name, oi.quantity, oi.price\n",
    "            FROM customers c\n",
    "            JOIN orders o ON c.id = o.customer_id\n",
    "            JOIN order_items oi ON o.id = oi.order_id\n",
    "            JOIN products p ON oi.product_id = p.id\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run all tests\n",
    "results = []\n",
    "for scenario in test_scenarios:\n",
    "    print(f\"\\n{'='*80}\\nTesting: {scenario['name']}\\n{'='*80}\")\n",
    "    \n",
    "    sql, judge = test_sql_question(\n",
    "        question=scenario['question'],\n",
    "        expected_sql=scenario['expected_sql']\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        \"name\": scenario['name'],\n",
    "        \"passed\": judge.passed if judge else None,\n",
    "        \"scores\": judge.scores if judge else None\n",
    "    })\n",
    "    \n",
    "    # Wait between tests to avoid rate limiting\n",
    "    time.sleep(2)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "for result in results:\n",
    "    status = \"PASSED\" if result['passed'] else \"FAILED\" if result['passed'] is not None else \"NOT EVALUATED\"\n",
    "    print(f\"{result['name']}: {status}\")\n",
    "    if result['scores']:\n",
    "        avg_score = (result['scores'].accuracy + result['scores'].relevance + \n",
    "                    result['scores'].completeness + result['scores'].hallucination) / 4\n",
    "        print(f\"  Average Score: {avg_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Test Cases to YAML\n",
    "\n",
    "Convert your tested scenarios into YAML format for the evaluation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Create YAML structure for new test cases\n",
    "def create_yaml_test(name, question, expected_sql, judge_criteria=None):\n",
    "    test = {\n",
    "        \"name\": name,\n",
    "        \"question\": question,\n",
    "        \"sql\": expected_sql\n",
    "    }\n",
    "    \n",
    "    if judge_criteria:\n",
    "        test[\"judge_criteria\"] = judge_criteria\n",
    "    \n",
    "    return test\n",
    "\n",
    "# Create a test suite\n",
    "test_suite = {\n",
    "    \"schema_file\": \"schema/schema.json\",\n",
    "    \"default_judge_criteria\": {\n",
    "        \"check_accuracy\": True,\n",
    "        \"check_relevance\": True,\n",
    "        \"check_completeness\": True,\n",
    "        \"check_hallucination\": True\n",
    "    },\n",
    "    \"tests\": [\n",
    "        create_yaml_test(\n",
    "            name=\"customer_orders_count\",\n",
    "            question=\"How many orders does each customer have?\",\n",
    "            expected_sql=\"SELECT c.name, COUNT(o.id) as order_count FROM customers c LEFT JOIN orders o ON c.id = o.customer_id GROUP BY c.id, c.name\"\n",
    "        ),\n",
    "        create_yaml_test(\n",
    "            name=\"revenue_by_month\",\n",
    "            question=\"Show monthly revenue for this year\",\n",
    "            expected_sql=\"SELECT DATE_FORMAT(order_date, '%Y-%m') as month, SUM(total_amount) as revenue FROM orders WHERE YEAR(order_date) = YEAR(CURRENT_DATE) GROUP BY month ORDER BY month\"\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display the YAML\n",
    "yaml_content = yaml.dump(test_suite, default_flow_style=False, sort_keys=False)\n",
    "print(\"Generated YAML for evaluation framework:\")\n",
    "print(\"-\" * 80)\n",
    "print(yaml_content)\n",
    "\n",
    "# Optionally save to file\n",
    "# with open('../evals/sql_agent/e2e/custom_tests.yaml', 'w') as f:\n",
    "#     yaml.dump(test_suite, f, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug SQL Pipeline Stages\n",
    "\n",
    "Test individual stages of the SQL pipeline for detailed debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agent.adapters.llm import LLM\n",
    "from src.agent.config import get_llm_config\n",
    "from src.agent.domain import sql_model\n",
    "import uuid\n",
    "\n",
    "def test_sql_pipeline_stages(question: str):\n",
    "    \"\"\"Test each stage of the SQL pipeline individually.\"\"\"\n",
    "    \n",
    "    q_id = f\"debug-{str(uuid.uuid4())}\"\n",
    "    sql_question = commands.SQLQuestion(question=question, q_id=q_id)\n",
    "    \n",
    "    llm = LLM(get_llm_config())\n",
    "    agent = sql_model.SQLBaseAgent(\n",
    "        question=sql_question,\n",
    "        kwargs={\"schema_info\": schema}\n",
    "    )\n",
    "    \n",
    "    print(f\"Testing SQL Pipeline for: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Stage 1: Pre-check\n",
    "    print(\"\\n1. PRE-CHECK STAGE\")\n",
    "    check_cmd = agent.update(sql_question)\n",
    "    check_response = llm.use(check_cmd.question, response_model=commands.GuardrailPreCheckModel)\n",
    "    print(f\"   Approved: {check_response.approved}\")\n",
    "    if not check_response.approved:\n",
    "        print(f\"   Reason: {check_response.reason}\")\n",
    "        return\n",
    "    \n",
    "    # Stage 2: Grounding\n",
    "    print(\"\\n2. GROUNDING STAGE\")\n",
    "    check_result = commands.SQLCheck(question=question, q_id=q_id, approved=True)\n",
    "    grounding_cmd = agent.update(check_result)\n",
    "    grounding_response = llm.use(grounding_cmd.question, response_model=commands.SQLGrounding)\n",
    "    \n",
    "    print(\"   Table Mappings:\")\n",
    "    for tm in grounding_response.table_mapping:\n",
    "        print(f\"     '{tm.question_term}' -> {tm.table_name} (confidence: {tm.confidence})\")\n",
    "    \n",
    "    print(\"   Column Mappings:\")\n",
    "    for cm in grounding_response.column_mapping:\n",
    "        print(f\"     '{cm.question_term}' -> {cm.table_name}.{cm.column_name} (confidence: {cm.confidence})\")\n",
    "    \n",
    "    # Update agent state\n",
    "    agent.construction.table_mapping = grounding_response.table_mapping\n",
    "    agent.construction.column_mapping = grounding_response.column_mapping\n",
    "    \n",
    "    # Stage 3: Filter\n",
    "    print(\"\\n3. FILTER STAGE\")\n",
    "    filter_cmd = agent.update(grounding_response)\n",
    "    filter_response = llm.use(filter_cmd.question, response_model=commands.SQLFilter)\n",
    "    \n",
    "    print(\"   Filter Conditions:\")\n",
    "    for cond in filter_response.conditions:\n",
    "        print(f\"     {cond.column} {cond.operator} {cond.value}\")\n",
    "    \n",
    "    agent.construction.conditions = filter_response.conditions\n",
    "    \n",
    "    # Stage 4: Join Inference\n",
    "    print(\"\\n4. JOIN INFERENCE STAGE\")\n",
    "    join_cmd = agent.update(filter_response)\n",
    "    join_response = llm.use(join_cmd.question, response_model=commands.SQLJoinInference)\n",
    "    \n",
    "    print(\"   Joins:\")\n",
    "    for join in join_response.joins:\n",
    "        print(f\"     {join.from_table}.{join.from_column} -> {join.to_table}.{join.to_column} ({join.join_type})\")\n",
    "    \n",
    "    agent.construction.joins = join_response.joins\n",
    "    \n",
    "    # Stage 5: Aggregation\n",
    "    print(\"\\n5. AGGREGATION STAGE\")\n",
    "    agg_cmd = agent.update(join_response)\n",
    "    agg_response = llm.use(agg_cmd.question, response_model=commands.SQLAggregation)\n",
    "    \n",
    "    print(f\"   Is Aggregation Query: {agg_response.is_aggregation_query}\")\n",
    "    if agg_response.aggregations:\n",
    "        print(\"   Aggregations:\")\n",
    "        for agg in agg_response.aggregations:\n",
    "            print(f\"     {agg.function}({agg.column}) as {agg.alias}\")\n",
    "    if agg_response.group_by_columns:\n",
    "        print(f\"   Group By: {', '.join(agg_response.group_by_columns)}\")\n",
    "    \n",
    "    agent.construction.aggregations = agg_response.aggregations\n",
    "    agent.construction.group_by_columns = agg_response.group_by_columns\n",
    "    agent.construction.is_aggregation_query = agg_response.is_aggregation_query\n",
    "    \n",
    "    # Stage 6: Construction\n",
    "    print(\"\\n6. CONSTRUCTION STAGE\")\n",
    "    construction_cmd = agent.update(agg_response)\n",
    "    construction_response = llm.use(construction_cmd.question, response_model=commands.SQLConstruction)\n",
    "    \n",
    "    print(\"   Generated SQL:\")\n",
    "    print(f\"   {construction_response.sql_query}\")\n",
    "    \n",
    "    return construction_response.sql_query\n",
    "\n",
    "# Test the pipeline\n",
    "sql = test_sql_pipeline_stages(\"Show me the top 5 products by revenue last quarter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Test Existing YAML Fixtures\n",
    "\n",
    "Load existing test cases from YAML files to verify they work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.utils import load_yaml_fixtures\n",
    "\n",
    "# Load existing fixtures\n",
    "existing_fixtures = load_yaml_fixtures(Path(\"../evals/sql_agent\"), \"e2e\")\n",
    "\n",
    "print(f\"Found {len(existing_fixtures)} existing test cases:\\n\")\n",
    "\n",
    "# Display first few test cases\n",
    "for i, (name, fixture) in enumerate(list(existing_fixtures.items())[:3]):\n",
    "    print(f\"{i+1}. {name}\")\n",
    "    print(f\"   Question: {fixture['question']}\")\n",
    "    print(f\"   Expected SQL: {fixture['sql'][:100]}...\" if len(fixture['sql']) > 100 else f\"   Expected SQL: {fixture['sql']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a specific fixture\n",
    "if existing_fixtures:\n",
    "    test_name = list(existing_fixtures.keys())[0]\n",
    "    fixture = existing_fixtures[test_name]\n",
    "    \n",
    "    print(f\"Testing fixture: {test_name}\\n\")\n",
    "    \n",
    "    sql, judge = test_sql_question(\n",
    "        question=fixture['question'],\n",
    "        expected_sql=fixture['sql']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Testing\n",
    "\n",
    "Test the performance of SQL generation with different complexity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define queries of increasing complexity\n",
    "complexity_tests = [\n",
    "    {\"level\": \"Simple\", \"question\": \"Show all customers\"},\n",
    "    {\"level\": \"Filter\", \"question\": \"Show customers from California\"},\n",
    "    {\"level\": \"Join\", \"question\": \"Show orders with customer names\"},\n",
    "    {\"level\": \"Aggregate\", \"question\": \"Count orders by customer\"},\n",
    "    {\"level\": \"Complex\", \"question\": \"Show top 10 customers by total order value with their most recent order date\"}\n",
    "]\n",
    "\n",
    "# Run performance tests\n",
    "performance_results = []\n",
    "\n",
    "for test in complexity_tests:\n",
    "    print(f\"Testing {test['level']} query...\")\n",
    "    \n",
    "    # Time the execution\n",
    "    start_time = time.time()\n",
    "    sql, _ = test_sql_question(test['question'])\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    performance_results.append({\n",
    "        \"level\": test['level'],\n",
    "        \"time\": execution_time,\n",
    "        \"sql_length\": len(sql) if sql else 0\n",
    "    })\n",
    "    \n",
    "    # Wait between tests\n",
    "    time.sleep(2)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Execution time\n",
    "levels = [r['level'] for r in performance_results]\n",
    "times = [r['time'] for r in performance_results]\n",
    "ax1.bar(levels, times)\n",
    "ax1.set_xlabel('Query Complexity')\n",
    "ax1.set_ylabel('Execution Time (seconds)')\n",
    "ax1.set_title('SQL Generation Time by Complexity')\n",
    "\n",
    "# SQL length\n",
    "lengths = [r['sql_length'] for r in performance_results]\n",
    "ax2.bar(levels, lengths)\n",
    "ax2.set_xlabel('Query Complexity')\n",
    "ax2.set_ylabel('SQL Query Length (characters)')\n",
    "ax2.set_title('Generated SQL Length by Complexity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(f\"Average execution time: {np.mean(times):.2f}s\")\n",
    "print(f\"Min/Max execution time: {np.min(times):.2f}s / {np.max(times):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing\n",
    "\n",
    "Use this cell to interactively test SQL questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing - modify the question and run the cell\n",
    "interactive_question = \"What are the total sales by product category for last month?\"\n",
    "\n",
    "sql, _ = test_sql_question(interactive_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Session Results\n",
    "\n",
    "Save your testing session results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save session results\n",
    "session_results = {\n",
    "    \"timestamp\": int(time.time()),\n",
    "    \"tests_run\": len(performance_results) if 'performance_results' in locals() else 0,\n",
    "    \"model_info\": {\n",
    "        \"llm_model_id\": os.environ.get(\"llm_model_id\", \"unknown\"),\n",
    "        \"llm_temperature\": os.environ.get(\"llm_temperature\", \"unknown\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "results_file = f\"sql_eval_session_{session_results['timestamp']}.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(session_results, f, indent=2)\n",
    "\n",
    "print(f\"Session results saved to: {results_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}